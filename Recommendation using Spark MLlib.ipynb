{"cells":[{"cell_type":"markdown","source":["# Recommendation System with Spark ML\n\nIn this notebook we explore creating a movie recommendation system using Spark ML. We will work with the MovieLens 20M dataset (http://grouplens.org/datasets/movielens). This dataset consists of more than 20 million ratings of approximately 27,000 movies made by 138,000 MovieLens users who joined MovieLens in 2015. Includes tag genome data with 12 million relevance scores across 1,100 tags.MovieLens is a recommender system and virtual community website that recommends movies for its users to watch, based on their film preferences using collaborative filtering. This benchmark dataset was released April 2015.\n\nSpark ML supports an implementation of matrix factorization for collaborative filtering. Matrix factorization models have consistently shown to perform extremely well for collaborative filtering. The type of matrix factorization we will explore in this notebook is called explicit matrix factorization. In explicit matrix factorization, preferences provided by users themselves are utilized - as contrasted with implicit matrix factorization, where only implicit feedback (e.g. views, clicks, purchases, likes, shares etc.) is utilized. Collaborative filtering aims to fill in the missing entries of a user-item (in the case of movie recommendations, this consists of user and movie IDs) association matrix in which users (userID) and items (movieID) are described by a small set of latent factors that can be used to predict missing entries. Spark ML uses the Alternating Least Squares (ALS) algorithm to learn these latent factors for this matix factorization problem. ALS works by iteratively solving a series of least square regression problems to derive a model.\n\n![Factorization Graphic](https://encrypted-tbn1.gstatic.com/images?q=tbn:ANd9GcSID6SakBUeYVGD4VUJ06oJwnEtqeXfnicgBWu5n7fIDTY6HsHooA)"],"metadata":{}},{"cell_type":"markdown","source":["## Verify Spark version and existence of Spark context"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql import SparkSession\nspark = SparkSession.builder.appName(\"Movie Recommendation Engine using Spark\").config(\"spark.some.config.option\",\"some-value\").getOrCreate()\nsc = spark.sparkContext"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":["## Import Spark libraries"],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.tuning import ParamGridBuilder,CrossValidator\nfrom pyspark.sql import SQLContext \nfrom pyspark.ml.evaluation import RegressionEvaluator\nfrom pyspark.ml.recommendation import ALS\nfrom pyspark.sql import Row\nimport pyspark.sql.functions\nfrom pyspark.sql.functions import lit\nimport sys\n"],"metadata":{"collapsed":true},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":["## Create the Spark SQL Context"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql import SQLContext\nsqlContext = SQLContext(sc)"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":["## Download and load the MovieLens 20 Million dataset"],"metadata":{}},{"cell_type":"markdown","source":["## Read in Ratings Dataset\n### Ratings File Description\n\nAll ratings are contained in the file “ratings.dat” and are in the following format:\n\n    UserID::MovieID::Rating::Timestamp\n\n    UserIDs range between 1 and 138,000\n    MovieIDs range between 1 and 27,000\n    Ratings are made on a 5-star scale (whole-star ratings only)\n    Timestamp is represented in seconds \n    \nEach user has at least 20-30 ratings"],"metadata":{}},{"cell_type":"markdown","source":["## Read contents of \"ratings.dat\" into a dataframe"],"metadata":{}},{"cell_type":"code","source":["df = spark.read.csv(\"/FileStore/tables/ratings.csv\" , header= 'true')"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":["## Examine the ratings dataframe schema"],"metadata":{}},{"cell_type":"code","source":["df.printSchema()"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["df.head()"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":["## Examine the total ratings dataframe record count"],"metadata":{}},{"cell_type":"code","source":["df.count()"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":["## Now lets us find standard deviation, mean, minimum & maximum values"],"metadata":{}},{"cell_type":"code","source":["df.describe().show()"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":["## Now lets see a sample of the Ratings DataFrame"],"metadata":{}},{"cell_type":"code","source":["df.show(3)"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":["# Convert all datatypes into integer values"],"metadata":{}},{"cell_type":"code","source":["ratings = df.select('userId','movieId','rating')\nfrom pyspark.sql.types import IntegerType\nratings=ratings.withColumn(\"userId\", ratings[\"userId\"].cast(IntegerType()))\nratings=ratings.withColumn(\"movieId\", ratings[\"movieId\"].cast(IntegerType()))\nratings=ratings.withColumn(\"rating\", ratings[\"rating\"].cast(IntegerType()))\nratings.dtypes"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":["## Now let us take a small fraction of the dataset for our recommendation engine"],"metadata":{}},{"cell_type":"code","source":["ratings.sample(fraction=0.1, seed=4).count()"],"metadata":{"scrolled":true},"outputs":[],"execution_count":24},{"cell_type":"markdown","source":["## Show sample number of ratings per user"],"metadata":{}},{"cell_type":"code","source":["grouped_ratings = ratings.groupBy(\"userId\").count().withColumnRenamed(\"count\", \"No. of ratings\")\ngrouped_ratings.show(10)"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":["## Show the number of users in the dataset is approximately 138500"],"metadata":{}},{"cell_type":"code","source":["print(grouped_ratings.count())"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"markdown","source":["## Read in Movies Dataset\n### Movies File Description\n\nMovie information is in the file “movies.dat” and is in the following format:\n\n    movieId::Title::Genres\n\nTitles are identical to titles provided by the IMDb and include the year of release\n\nGenres are pipe-separated and are selected from the following genres:\n* Action\n* Adventure\n* Animation\n* Children's\n* Comedy\n* Crime\n* Documentary\n* Drama\n* Fantasy\n* Film-Noir\n* Horror\n* Musical\n* Mystery\n* Romance\n* Sci-Fi\n* Thriller\n* War\n* Western\n\nSome MovieIDs do not correspond to a movie due to accidental duplicate entries and/or test entries. Movies are mostly entered by hand, so errors and inconsistencies may exist"],"metadata":{}},{"cell_type":"markdown","source":["## Read contents of \"movies.dat\" and show sample content"],"metadata":{}},{"cell_type":"code","source":["movies = spark.read.csv(\"/FileStore/tables/movies.csv\",header=\"true\")\nfrom pyspark.sql.types import IntegerType\nmovies=movies.withColumn(\"movieId\", movies[\"movieId\"].cast(IntegerType()))\nmovies.show(5)"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"markdown","source":["## Show the number of movies in the dataset is approximately 27000"],"metadata":{}},{"cell_type":"code","source":["movies.count()"],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"markdown","source":["## Split Ratings data into Training (80%) and Test (20%) datasets"],"metadata":{}},{"cell_type":"code","source":["train, test= ratings.randomSplit([0.8,0.2], seed=26)\n"],"metadata":{"collapsed":true},"outputs":[],"execution_count":35},{"cell_type":"markdown","source":["## Show resulting Ratings dataset counts"],"metadata":{}},{"cell_type":"code","source":["print(\"Total no of records in ratings the dataset sample: \")\nprint(ratings.count())\nprint(\"Total no of train ratings records in the dataset sample:  \")\nprint(train.count())\nprint(\"Total no of test ratings records in the dataset sample: \")\nprint(test.count())"],"metadata":{},"outputs":[],"execution_count":37},{"cell_type":"markdown","source":["## Show sample of Ratings Training dataset"],"metadata":{}},{"cell_type":"code","source":["train.sample(withReplacement=False, fraction=0.0001, seed=3).show(5)"],"metadata":{},"outputs":[],"execution_count":39},{"cell_type":"markdown","source":["## Show sample of Ratings Test dataset"],"metadata":{}},{"cell_type":"code","source":["test.sample(withReplacement=False, fraction=0.0001, seed=3).show(5)"],"metadata":{"scrolled":true},"outputs":[],"execution_count":41},{"cell_type":"markdown","source":["## Build the recommendation model on the training data using ALS"],"metadata":{}},{"cell_type":"code","source":["als = ALS().setMaxIter(10).setRegParam(0.01).setUserCol(\"userId\").setItemCol(\"movieId\").setRatingCol(\"rating\")\nmodel = als.fit(train)"],"metadata":{"collapsed":true},"outputs":[],"execution_count":43},{"cell_type":"markdown","source":["## ALS Model Parameters\n\n### Let's take a look at all the paramaters available for ALS, the default values of those parameters which we did not change, and valide the values of those parameters set above.\n\n#### We specifically set\n\n    MaxIter (maximum number of iterations) = 10 (which is the default)\n    RegParam (regularization parameter) = 0.01 (default is 0.1)\n    UserCol (column name for user ids) = “userID” (default is \"user\")\n    ItemCol (column name for item ids) = “movieID” (default is \"item\")\n    RatingCol (column for ratings) = “rating” (which is the default)"],"metadata":{}},{"cell_type":"markdown","source":["## Show the ALS explained parameters"],"metadata":{}},{"cell_type":"code","source":["print(als.explainParams())"],"metadata":{},"outputs":[],"execution_count":46},{"cell_type":"markdown","source":["## Run the model against the Test data and show a sample of the predictions"],"metadata":{}},{"cell_type":"code","source":["predictions = model.transform(test).na.drop()\npredictions.show(10)"],"metadata":{},"outputs":[],"execution_count":48},{"cell_type":"markdown","source":["## Evaluate the model by computing the RMSE on the test data"],"metadata":{}},{"cell_type":"markdown","source":["The Spark ML evaluator for regression, RegressionEvaluator, expects two input columns: prediction and label. RegressionEvaluator supports “rmse” (default), “mse”, “r2”, and “mae”. \n\nWe will use RMSE, which is the square root of the average of the square of all of the error. \nRMSE is an excellent general purpose error metric for numerical predictions."],"metadata":{}},{"cell_type":"code","source":["evaluator = RegressionEvaluator().setMetricName(\"rmse\").setLabelCol(\"rating\").setPredictionCol(\"prediction\")\nrmse = evaluator.evaluate(predictions)\nprint(\"Root Mean Square Error : \")\nprint(rmse)"],"metadata":{},"outputs":[],"execution_count":51},{"cell_type":"markdown","source":["## Show that a smaller value of rmse is better\n\nThis is obviously the case since RMSE is an aggregation of all the error. Thus evaluator.isLargerBetter should be 'false'."],"metadata":{}},{"cell_type":"code","source":["evaluator.isLargerBetter()"],"metadata":{},"outputs":[],"execution_count":53},{"cell_type":"markdown","source":["## Tune the Model\nBuild a Parameter Grid specifying what parameters and values will be evaluated in order to determine the best combination.\n\nSpark ML algorithms provide many hyperparameters for tuning models. These hyperparameters are distinct from the model parameters being optimized by ML itself. Hyperparameter tuning is accomplished by choosing the best set of parameters based on model performance on test data that the model was not trained with. All combinations of hyperparameters specified will be tried in order to find the one that leads to the model with the best evaluation result.\n\nIn this example, we will only be evaluating the ALS regularization parameter, regParam. In machine learning, regularization refers to a process of introducing additional information in order to prevent overfitting."],"metadata":{}},{"cell_type":"code","source":["paramGrid = ParamGridBuilder().addGrid(als.regParam, [0.01, 0.1]).build()"],"metadata":{"collapsed":true},"outputs":[],"execution_count":55},{"cell_type":"markdown","source":["## Create a cross validator to tune the model with the defined parameter grid\n\nCross-validation attempts to fit the underlying estimator with user-specified combinations of parameters, cross-evaluate the fitted models, and output the best one.\n\nIn k-fold cross-validation, the original sample is randomly partitioned into k equal sized subsamples. Of the k subsamples, a single subsample is retained as the validation data for testing the model, and the remaining k − 1 subsamples are used as training data. The cross-validation process is then repeated k times (the folds), with each of the k subsamples used exactly once as the validation data. The k results from the folds can then combined to produce a single estimation."],"metadata":{}},{"cell_type":"code","source":["cv = CrossValidator().setEstimator(als).setEvaluator(evaluator).setEstimatorParamMaps(paramGrid).setNumFolds(2)"],"metadata":{"collapsed":true},"outputs":[],"execution_count":57},{"cell_type":"markdown","source":["## Validate the parameter grid values"],"metadata":{}},{"cell_type":"code","source":["cv.getEstimatorParamMaps()\n"],"metadata":{},"outputs":[],"execution_count":59},{"cell_type":"markdown","source":["## Cross-evaluate to find the best model\n\nusing the RMSE evaluator and hyperparameters specified in the parameter grid"],"metadata":{}},{"cell_type":"code","source":["cvModel = cv.fit(train)"],"metadata":{"collapsed":true},"outputs":[],"execution_count":61},{"cell_type":"code","source":["predictions = cvModel.transform(test).na.drop()\npredictions.show(10,False)"],"metadata":{},"outputs":[],"execution_count":62},{"cell_type":"code","source":["print(evaluator.evaluate(cvModel.transform(test).na.drop()))"],"metadata":{},"outputs":[],"execution_count":63},{"cell_type":"markdown","source":["## Now lets use our model to recommend movies to a random user\nFor this example we will be recommending movies to user with userID = 237."],"metadata":{}},{"cell_type":"code","source":["userId = 237"],"metadata":{"collapsed":true},"outputs":[],"execution_count":65},{"cell_type":"markdown","source":["## Create a DataFrame with the movies that user 237 has rated\nFirst let's take a look at user 237's ratings in the ratings dataset."],"metadata":{}},{"cell_type":"code","source":["movies_watched = ratings.filter(ratings.userId == userId)\nmovies_watched.show(10,False)"],"metadata":{},"outputs":[],"execution_count":67},{"cell_type":"markdown","source":["## Calculate  user 237's minimum, maximum and average movie rating"],"metadata":{}},{"cell_type":"code","source":["movies_watched.describe().show()"],"metadata":{},"outputs":[],"execution_count":69},{"cell_type":"markdown","source":["## Show user 237's top 10 rated movies (with movie title, genre, and rating)\nTo do this, we must join the ratings dataset with the movies dataset. The ratings dataset only contains the movieID. Only the movies dataset contains the movie title and genre.\n\nWe are joining the ratings and movies datasets based on the common movieID column, filtering on userID 237, and sorting in descending order by rating."],"metadata":{}},{"cell_type":"code","source":["a = ratings.alias('a')\nb = movies.alias('b')"],"metadata":{},"outputs":[],"execution_count":71},{"cell_type":"code","source":["a.filter(ratings.userId == userId).join(b, a.movieId == b.movieId).select(a.userId, a.movieId, b.title, b.genres, a.rating).sort(a.rating,ascending=False).show(10,False)"],"metadata":{},"outputs":[],"execution_count":72},{"cell_type":"markdown","source":["## Determining what movies user 237 has not already watched and rated so that we can make new movie recommendations\n\nIn order to make new movie recommendations from the list of movies in the movies dataset, we must first figure out which movies user 237 has not already watched."],"metadata":{}},{"cell_type":"code","source":["movies_notwatched = ratings.filter(ratings.userId != userId)\nmovies_notwatched.sample(False, 0.0001, seed=0).show(10,False)\nprint(\"Total movie ratings in the ratings dataset not rated and not watched by user 237 = \")\nprint(movies_notwatched.count())"],"metadata":{},"outputs":[],"execution_count":74},{"cell_type":"markdown","source":["## Determining what movies user 237 has not already watched and rated so that we can make new movie recommendations - another attempt\n\nThis previous attempt at determining what movies user 237 has not already watched, did not really give us what we need. It simply provided all the movie ratings in the ratings dataset not rated by user 237. What we want, instead, is a list of all movies in the movies dataset that user 237 has not rated so that we can make new movie recommendations based on these movies which user 237 has not yet watched. We don't want to recommend movies that the user has already rated.\n\nIn order to do this, we will again need to join the ratings and movies datasets. However, unlike the the previous join we did, which was an inner join, this join needs to be an outer join as we want all the movies in the movies dataset that the user has not rated in the ratings dataset. Since the join order is ratings then movies, we specifically need to employ a right outer join. We will again use movieId as the join column, filter on userId 237"],"metadata":{}},{"cell_type":"code","source":["a = ratings.alias('a')\nb = movies.alias('b')\nmovies_notwatched_movieId = a.filter(ratings.userId == userId).join(b, a.movieId == b.movieId, 'right').filter(a.movieId.isNull()).select(b.movieId,b.title).sort(b.movieId,ascending = True)\nmovies_notwatched_movieId.show(10,False)"],"metadata":{},"outputs":[],"execution_count":76},{"cell_type":"markdown","source":["## Let's check dataframe we just created\nLet's check to see that the results of our right outer join make sense by looking at the row count of the resulting DataFrame. The number of movies not watched by the user from the movies dataset (the result of the right outer join) plus the number of movies rated by the user in the ratings dataset should equal the total number of movies in the movies dataset."],"metadata":{}},{"cell_type":"code","source":["print(\"Total number of movies = \")\nprint(movies.count())\nprint(\"Number of movies rated by user = \")\nprint(ratings.filter(ratings.userId == userId).count())\nprint(\"Number of movies NOT rated & NOT watched by user = \")\nprint(movies_notwatched_movieId.count())"],"metadata":{},"outputs":[],"execution_count":78},{"cell_type":"markdown","source":["## Create input DataFrame to use with the model to recommend new movies\nThe ALS algorithm we used above to build the recommendation model requires two inputs for making predictions - userID and movieId. Look back at the ratings dataset we used earlier to test the model to confirm this.\n\nRemember that the rating column in the ratings dataset is only used for training and evaluating the model, not for making predicitons.\n\nIn order to make the DataFrame resulting from the right outer join ready for input into the model to make movie predicitons for user 237, we need to add a userId column to the DataFrame with userId set to 237 for every movieId. As this is the dataset that predicitons will be made on, it must contain movieId and userID columns with values for the movies (those which the user has not already watched and rated - the result of the right outer join) and the userId (in this case 237) for whom we are making recommendations."],"metadata":{}},{"cell_type":"code","source":["\ndata_userId = movies_notwatched_movieId.withColumn(\"userId\", lit(userId))\ndata_userId.show(10, False)\nprint(\" Total number of movies not watched and not rated by user 237 = \")\nprint(data_userId.count())\n"],"metadata":{},"outputs":[],"execution_count":80},{"cell_type":"markdown","source":["## Generate top 10 movie recommendations for each user"],"metadata":{}},{"cell_type":"code","source":["userRecs = model.recommendForAllUsers(10)\nprint(userRecs.show())"],"metadata":{},"outputs":[],"execution_count":82},{"cell_type":"markdown","source":["##  Generate top 10 user recommendations for each movie"],"metadata":{}},{"cell_type":"code","source":["movieRecs = model.recommendForAllItems(10)\nprint(movieRecs.show())"],"metadata":{},"outputs":[],"execution_count":84},{"cell_type":"markdown","source":["## Generate top 10 movie recommendations for userID 237"],"metadata":{}},{"cell_type":"code","source":["#users = ratings.select(als.getUserCol()).distinct().limit(3)\nusers = ratings.filter(ratings.userId==userId)\nuserSubsetRecs = model.recommendForUserSubset(users,10)\nprint(\"Top 10 Movie Recommendations for 237 user ID is as follows in [MovieID, Prediction] format :\")\nprint(userSubsetRecs.select(\"recommendations\").show(10,False))\n"],"metadata":{},"outputs":[],"execution_count":86},{"cell_type":"markdown","source":["##  Conclusion:\nAs you can find above the top 10 movie recommendations for user 237 depicted in the form of Movie Id and Prediction Score\n\n##### MovieId             Prediction\n\n79410  ||  7.897084\n\n73529  ||  7.3702297\n\n107434 ||  6.6337686 \n\n116155  || 6.584987\n\n66927  ||  6.52591\n\n80480  ||  6.5221\n\n88313   || 6.5021305\n\n103753  || 6.3608503\n\n109253  || 6.3608503\n\n113790  || 6.3608503"],"metadata":{}},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":88}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"mimetype":"text/x-python","name":"python","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3},"version":"3.6.4","nbconvert_exporter":"python","file_extension":".py"},"name":"MovieRecommendations","notebookId":4362636156699369},"nbformat":4,"nbformat_minor":0}
